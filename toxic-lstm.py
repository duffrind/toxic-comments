import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils
import pandas as pd

df = pd.read_csv("./toxic/train.csv")
training = ' '.join(df[df.toxic==1].comment_text.str.lower().values)

ignore_chars = {'<', '^', '`', '\x93', '\x94', 'Â¢', 'Â£', 'Â¤', 'Â¦', 'Â§', 'Â¨', 'Â©',
               '\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â´', 'Â·', 'Â¸', 'Â½', 'Â¿', 'ÃŸ', 'Ã ',
               'Ã¡', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ãª', 'Ã­', 'Ã¯', 'Ã±', 'Ã³', 'Ã¶', 'Ã¹',
               'Ãº', 'Ã¼', 'Ã¾', 'Ä…', 'Ä‡', 'Ä‘', 'Ä—', 'Ä›', 'Ä£', 'Ä¥', 'Ä§', 'Ä±', 'Å„',
               'Å†', 'Å', 'Å“', 'Å›', 'ÅŸ', 'Å¡', 'Å£', 'Å©', 'Åµ', 'Å·', 'Å¼', 'Æ’', 'Ç”',
               'È³', 'Ì‡', 'Î¬', 'Î¯', 'Î±', 'Î³', 'Î´', 'Îµ', 'Î·', 'Î¸', 'Î¹', 'Îº', 'Î»',
               'Î¼', 'Î½', 'Î¿', 'Ï€', 'Ï', 'Ï‚', 'Ïƒ', 'Ï„', 'Ï…', 'Ï†', 'Ï‰', 'ÏŒ', 'Ï',
               'ÏŽ', 'Ð°', 'Ð±', 'Ð²', 'Ð³', 'Ð´', 'Ðµ', 'Ð¶', 'Ð¸', 'Ð¹', 'Ðº', 'Ð»', 'Ð¼',
               'Ð½', 'Ð¾', 'Ð¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ†', 'Ñ‡', 'Ñ‰', 'ÑŠ', 'Ñ‹',
               'ÑŒ', 'Ñ', 'Ñ™', 'Ö¼', '×', '×‘', '×•', '×™', '×›', '×œ', '×ž', 'Ø§', 'Øª',
               'Ø³', 'Ø·', 'Ø¹', 'Ù', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ùˆ', 'ÙŠ', 'Ú†', 'Úœ', 'Ú¬',
               'Ú°', 'Úµ', '\u06dd', 'Ûž', 'Û¬', 'Ûµ', 'Û¸', 'Û»', 'Û¾', 'Ý“', 'Ý—', 'Ýœ',
               'ÝŸ', 'Ý¡', 'Ý£', 'Ý­', 'à¶š', 'à¶­', 'à¶³', 'à¶»', 'à·€', 'à·Š', 'à·”', 'á›', 'áµ½',
               'á¸Ÿ', 'á¸»', 'á¹ƒ', 'á¹—', 'á¹£', 'á¹¯', 'â€“', 'â€˜', 'â€œ', 'â€', 'â€ž', 'â€ ', 'â€¢',
               'â€¦', '\u2060', 'â‚¡', 'â‚¨', 'â‚©', 'â‚ª', 'â‚¬', 'â‚­', 'â‚³', 'â‚µ', 'â„–', 'â„¢',
               'â„³', 'â…ž', 'â†', 'â†‘', 'â†’', 'â†”', 'â†¨', 'â‡’', 'â‡”', 'âˆ‚', 'âˆ†', 'âˆ‡', 'âˆ’',
               'âˆš', 'âˆž', 'âˆ«', 'â‰ˆ', 'â‰ ', 'â‰¤', 'âŠ•', 'â”€', 'â•Ÿ', 'â•¢', 'â•¦', 'â–º', 'â—„',
               'â˜…', 'â˜Ž', 'â˜', 'â˜¥', 'â˜­', 'â˜º', 'â˜»', 'â˜¼', 'â™ ', 'â™£', 'â™¥', 'â™¦', 'â™ª',
               'â™«', 'âœ„', 'âœ‰', 'âœ‹', 'âœ', 'âœŽ', 'âœ½', 'â', 'âž', 'âž¨', 'âŸ²', 'ãƒ„', 'å¦ˆ',
               'å­¦', 'å½±', 'æƒ‘', 'æ­¦', 'æ°¸', 'çƒ‚', 'çš„', 'çµ¡', 'è€…', 'è‡­', 'è¦‹', 'è¨£', 'è¿·',
               'é€£', '\ufeff', 'ï¼Ž', 'ï½', 'ï½ƒ', 'ï½‹', 'ï½Œ', 'ï½', 'ï½Ž', 'ï½', 'ï½”', 'ï½—',
               'ðŸ¼', 'ðŸ‘', 'ðŸ’©', 'ðŸ˜‚', 'ðŸ˜„', 'ðŸ˜Š'}

training = ''.join([ch if ch not in ignore_chars else '\u2600' for ch in training])

chars = sorted(list(set(training)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = {v: k for k, v in char_to_int.items()}

n_chars = len(training)
n_vocab = len(chars)
print("Total Characters: ", n_chars)
print("Total Vocab: ", n_vocab)

seq_length = 1
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
    seq_in = training[i:i + seq_length]
    seq_out = training[i + seq_length]
    dataX.append([char_to_int[char] for char in seq_in])
    dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)
print("Total Patterns: ", n_patterns)

X = np.reshape(dataX, (n_patterns, seq_length, 1))
X = X / float(n_vocab)
y = np_utils.to_categorical(dataY)

# define the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')

filepath="./models/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]

model.fit(X, y, epochs=10, batch_size=256, callbacks=callbacks_list)
