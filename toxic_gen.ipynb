{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramTCG():\n",
    "\n",
    "    def __init__(self,\n",
    "                 path:str='./toxic/train.csv',\n",
    "                 pos_flags:List[str]=['toxic'],\n",
    "                 neg_flags:List[str]=['identity_hate']):\n",
    "        '''\n",
    "        pos_flags: Only use comments where pos_flags are 1\n",
    "        neg_flags: Only use comments where neg_flags are 0\n",
    "        Flags can be 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    "        '''\n",
    "        df = pd.read_csv(path)\n",
    "        X = df[np.all(df[pos_flags]==1, 1)&np.all(df[neg_flags]==0, 1)].comment_text\n",
    "        X = X.apply(lambda x: nltk.tokenize.ToktokTokenizer().tokenize(x.lower()))\n",
    "\n",
    "        self._convert_text(X)\n",
    "\n",
    "    def _convert_text(self, X):\n",
    "        self._trigrams = dict()\n",
    "\n",
    "        for s in X.values:\n",
    "            s = ['<<START2>>', '<<START1>>'] + s + ['<<END1>>', '<<END2>>']\n",
    "            for a, b, c in zip(s, s[1:], s[2:]):\n",
    "                if a in self._trigrams:\n",
    "                    if b in self._trigrams[a]:\n",
    "                        self._trigrams[a][b][c] += 1\n",
    "                    else:\n",
    "                        self._trigrams[a][b] = Counter({c: 1})\n",
    "                else:\n",
    "                    self._trigrams[a] = {b: Counter({c: 1})}\n",
    "\n",
    "        for k, v in self._trigrams.items():\n",
    "            for k2, v2 in v.items():\n",
    "                s = sum(v2.values())\n",
    "                for k3, v3 in v2.items():\n",
    "                    self._trigrams[k][k2][k3] /= s\n",
    "\n",
    "    def _format_comment(self, comment:str):\n",
    "        comment = re.sub(r\" (?=[.\\\"'?!,)-])\", \"\", comment)\n",
    "        comment = re.sub(r\"(?<=[']) \", \"\", comment)\n",
    "\n",
    "        return comment\n",
    "\n",
    "    def generate_comment(self, random_start:bool=False):\n",
    "        comment = []\n",
    "        while comment == []:\n",
    "            if not random_start:\n",
    "                a, b = '<<START2>>', '<<START1>>'\n",
    "            else:\n",
    "                a, b = \"\", \"<<END1>>\"\n",
    "                while b == \"<<END1>>\":\n",
    "                    a = np.random.choice(list(self._trigrams.keys()))\n",
    "                    b = np.random.choice(list(self._trigrams[a].keys()))\n",
    "\n",
    "            c = np.random.choice(list(self._trigrams[a][b].keys()), p=list(self._trigrams[a][b].values()))\n",
    "\n",
    "            while c != '<<END1>>':\n",
    "                comment.append(c)\n",
    "                a = b\n",
    "                b = c\n",
    "                c = np.random.choice(list(self._trigrams[a][b].keys()), p=list(self._trigrams[a][b].values()))\n",
    "        comment = ' '.join(comment)\n",
    "\n",
    "        return self._format_comment(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcg = TrigramTCG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is about a battle by personal attack on you to make disappear any kind of horse-shit-for-brains dumbass would write on my dickle you can fuck my momma'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcg.generate_comment(random_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  4530787\n",
      "Total Vocab:  73\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./toxic/train.csv\")\n",
    "training = ' '.join(df[df.toxic==1].comment_text.str.lower().values)\n",
    "\n",
    "ignore_chars = {'<', '^', '`', '\\x93', '\\x94', '¢', '£', '¤', '¦', '§', '¨', '©',\n",
    "               '\\xad', '®', '¯', '°', '±', '²', '´', '·', '¸', '½', '¿', 'ß', 'à',\n",
    "               'á', 'ä', 'å', 'æ', 'ç', 'è', 'ê', 'í', 'ï', 'ñ', 'ó', 'ö', 'ù',\n",
    "               'ú', 'ü', 'þ', 'ą', 'ć', 'đ', 'ė', 'ě', 'ģ', 'ĥ', 'ħ', 'ı', 'ń',\n",
    "               'ņ', 'ō', 'œ', 'ś', 'ş', 'š', 'ţ', 'ũ', 'ŵ', 'ŷ', 'ż', 'ƒ', 'ǔ',\n",
    "               'ȳ', '̇', 'ά', 'ί', 'α', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ',\n",
    "               'μ', 'ν', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'ω', 'ό', 'ύ',\n",
    "               'ώ', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'и', 'й', 'к', 'л', 'м',\n",
    "               'н', 'о', 'п', 'р', 'с', 'т', 'у', 'х', 'ц', 'ч', 'щ', 'ъ', 'ы',\n",
    "               'ь', 'я', 'љ', 'ּ', 'א', 'ב', 'ו', 'י', 'כ', 'ל', 'מ', 'ا', 'ت',\n",
    "               'س', 'ط', 'ع', 'ف', 'ك', 'ل', 'م', 'ن', 'و', 'ي', 'چ', 'ڜ', 'ڬ',\n",
    "               'ڰ', 'ڵ', '\\u06dd', '۞', '۬', '۵', '۸', 'ۻ', '۾', 'ݓ', 'ݗ', 'ݜ',\n",
    "               'ݟ', 'ݡ', 'ݣ', 'ݭ', 'ක', 'ත', 'ඳ', 'ර', 'ව', '්', 'ු', 'ᛏ', 'ᵽ',\n",
    "               'ḟ', 'ḻ', 'ṃ', 'ṗ', 'ṣ', 'ṯ', '–', '‘', '“', '”', '„', '†', '•',\n",
    "               '…', '\\u2060', '₡', '₨', '₩', '₪', '€', '₭', '₳', '₵', '№', '™',\n",
    "               'ℳ', '⅞', '←', '↑', '→', '↔', '↨', '⇒', '⇔', '∂', '∆', '∇', '−',\n",
    "               '√', '∞', '∫', '≈', '≠', '≤', '⊕', '─', '╟', '╢', '╦', '►', '◄',\n",
    "               '★', '☎', '☏', '☥', '☭', '☺', '☻', '☼', '♠', '♣', '♥', '♦', '♪',\n",
    "               '♫', '✄', '✉', '✋', '✍', '✎', '✽', '❝', '❞', '➨', '⟲', 'ツ', '妈',\n",
    "               '学', '影', '惑', '武', '永', '烂', '的', '絡', '者', '臭', '見', '訣', '迷',\n",
    "               '連', '\\ufeff', '．', 'ａ', 'ｃ', 'ｋ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｔ', 'ｗ',\n",
    "               '🏼', '👍', '💩', '😂', '😄', '😊'}\n",
    "\n",
    "training = ''.join([ch if ch not in ignore_chars else '\\u2600' for ch in training])\n",
    "\n",
    "chars = sorted(list(set(training)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "n_chars = len(training)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  4530777\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = training[i:i + seq_length]\n",
    "    seq_out = training[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./models/weights-improvement-02-2.2415.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" elete my c \"\n",
      "amt dad mo the aasie toe toat and ia to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie t\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "out = ''\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    out += result\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe toee and the aatie to toe toee and the aatie to toe toee and the aatie to toe toee and the aatie"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    prediction = model.predict(np.reshape(pattern, (1, len(pattern), 1))/float(n_vocab)).argmax()\n",
    "    pred_char = int_to_char[prediction]\n",
    "    print(pred_char, end='')\n",
    "    pattern = pattern[1:] + [prediction]\n",
    "#     print(pattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
