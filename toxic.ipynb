{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments: TFIDF vs MaxEnt vs Polarity - David Duffrin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('toxic/train.csv')\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15294 159571\n"
     ]
    }
   ],
   "source": [
    "print(sum(comments.toxic == 1), len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[['comment_text', 'toxic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be looking at the comment_text and try to predict toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>87940</th>\n",
       "      <td>Dear Mr Moody,\\n\\nMy website does not discoura...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>Winx Club\\nYou recently edited the list of epi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135136</th>\n",
       "      <td>without any concern for Wikipedia's lazy admin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11405</th>\n",
       "      <td>There was a documentary shown on Mediacorp Cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23189</th>\n",
       "      <td>\"\\nI don't see the need for a \"\"lasting impact...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment_text  toxic\n",
       "toxic                                                                 \n",
       "0     87940   Dear Mr Moody,\\n\\nMy website does not discoura...      0\n",
       "      19563   Winx Club\\nYou recently edited the list of epi...      0\n",
       "      135136  without any concern for Wikipedia's lazy admin...      0\n",
       "      11405   There was a documentary shown on Mediacorp Cha...      0\n",
       "      23189   \"\\nI don't see the need for a \"\"lasting impact...      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_subset = comments.groupby('toxic').apply(lambda x: x.sample(15000, random_state=0))\n",
    "comments_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "comments_subset.comment_text = comments_subset.comment_text.apply(lambda w: Counter([ps.stem(word) for word in re.sub('[^a-z]', ' ', w.lower()).split() if word not in sw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>87940</th>\n",
       "      <td>{'dear': 1, 'mr': 1, 'moodi': 1, 'websit': 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>{'winx': 2, 'club': 2, 'recent': 1, 'edit': 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135136</th>\n",
       "      <td>{'without': 1, 'concern': 1, 'wikipedia': 1, '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11405</th>\n",
       "      <td>{'documentari': 1, 'shown': 1, 'mediacorp': 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23189</th>\n",
       "      <td>{'see': 1, 'need': 1, 'last': 1, 'impact': 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment_text  toxic\n",
       "toxic                                                                 \n",
       "0     87940   {'dear': 1, 'mr': 1, 'moodi': 1, 'websit': 2, ...      0\n",
       "      19563   {'winx': 2, 'club': 2, 'recent': 1, 'edit': 1,...      0\n",
       "      135136  {'without': 1, 'concern': 1, 'wikipedia': 1, '...      0\n",
       "      11405   {'documentari': 1, 'shown': 1, 'mediacorp': 1,...      0\n",
       "      23189   {'see': 1, 'need': 1, 'last': 1, 'impact': 1, ...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_toxic = len(comments_subset[comments_subset.toxic == 1])\n",
    "num_train = len(comments_subset)\n",
    "pc = num_toxic / num_train\n",
    "pnc = 1 - pc\n",
    "pc = -(pc*log(pc) + pnc*log(pnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for word_set in [w for w in comments_subset.comment_text]:\n",
    "    words |= set(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_info = Counter()\n",
    "train_set = [(x[2], x[1]) for x in comments_subset.itertuples()]\n",
    "\n",
    "for term in words:\n",
    "    pt = len([1 for _, x in train_set if term in x])/num_train\n",
    "    with_term = [x for x, text in train_set if term in text]\n",
    "    pc_t = len([1 for x in with_term if x == 1]) / len(with_term)\n",
    "    if pc_t == 0:\n",
    "        pc_t = 0.00001\n",
    "    elif pc_t == 1:\n",
    "        pc_t = 0.99999\n",
    "    t = pt * (pc_t*log(pc_t) + (1-pc_t)*log(1-pc_t))\n",
    "    without_term = [x for x, text in train_set if term not in text]\n",
    "    if len(without_term) == 0:\n",
    "        pc_nt = 0\n",
    "    else:\n",
    "        pc_nt = len([1 for x in without_term if x == 1]) / len(without_term)\n",
    "    if pc_nt == 0:\n",
    "        pc_nt = 0.00001\n",
    "    elif pc_nt == 1:\n",
    "        pc_nt = 0.99999\n",
    "    nt = (1-pt) * (pc_nt*log(pc_nt) + (1-pc_nt)*log(1-pc_nt))\n",
    "    term_info[term] = pc + t + nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = [x for x, _ in term_info.most_common(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'shit',\n",
       " 'articl',\n",
       " 'thank',\n",
       " 'ass',\n",
       " 'suck',\n",
       " 'bitch',\n",
       " 'stupid',\n",
       " 'pleas',\n",
       " 'talk']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that MaxEnt highly weights words that are toxic. This makes sense because you can see all kinds of words in toxic comments, however seeing toxic words in a nontoxic comment is highly unlikely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joint = [(y, 'pos' if x==1 else 'neg') for x, y in train_set]\n",
    "test_joint = train_joint[1::2]\n",
    "train_joint = train_joint[0::2]\n",
    "\n",
    "\n",
    "train_dict = []\n",
    "\n",
    "empty_dict = {word: 0 for word in feature_selection[:50]}\n",
    "\n",
    "for x, y in train_joint:\n",
    "    new_dict = empty_dict.copy()#dict()\n",
    "    for word in x:\n",
    "        if word in feature_selection[:50]:\n",
    "            new_dict[word] = x[word]\n",
    "    train_dict.append((new_dict, y))\n",
    "\n",
    "test_dict = []\n",
    "for x, y in test_joint:\n",
    "    new_dict = empty_dict.copy()#dict()\n",
    "    for word in x:\n",
    "        if word in feature_selection[:50]:\n",
    "            new_dict[word] = x[word]\n",
    "    test_dict.append((new_dict, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.66339        0.759\n",
      "             3          -0.63701        0.784\n",
      "             4          -0.61380        0.784\n",
      "             5          -0.59335        0.785\n",
      "             6          -0.57528        0.785\n",
      "             7          -0.55922        0.785\n",
      "             8          -0.54487        0.786\n",
      "             9          -0.53197        0.786\n",
      "            10          -0.52032        0.786\n",
      "            11          -0.50974        0.786\n",
      "            12          -0.50010        0.786\n",
      "            13          -0.49127        0.786\n",
      "            14          -0.48317        0.786\n",
      "            15          -0.47569        0.786\n",
      "            16          -0.46878        0.787\n",
      "            17          -0.46236        0.787\n",
      "            18          -0.45640        0.787\n",
      "            19          -0.45084        0.787\n",
      "            20          -0.44564        0.787\n",
      "            21          -0.44077        0.787\n",
      "            22          -0.43620        0.787\n",
      "            23          -0.43191        0.788\n",
      "            24          -0.42786        0.788\n",
      "            25          -0.42405        0.788\n",
      "            26          -0.42045        0.788\n",
      "            27          -0.41703        0.787\n",
      "            28          -0.41380        0.788\n",
      "            29          -0.41074        0.788\n",
      "            30          -0.40783        0.788\n",
      "            31          -0.40506        0.788\n",
      "            32          -0.40242        0.788\n",
      "            33          -0.39991        0.788\n",
      "            34          -0.39752        0.770\n",
      "            35          -0.39523        0.771\n",
      "            36          -0.39304        0.771\n",
      "            37          -0.39095        0.771\n",
      "            38          -0.38895        0.771\n",
      "            39          -0.38703        0.771\n",
      "            40          -0.38519        0.771\n",
      "            41          -0.38343        0.771\n",
      "            42          -0.38173        0.771\n",
      "            43          -0.38010        0.771\n",
      "            44          -0.37854        0.771\n",
      "            45          -0.37703        0.771\n",
      "            46          -0.37558        0.771\n",
      "            47          -0.37418        0.772\n",
      "            48          -0.37284        0.772\n",
      "            49          -0.37154        0.772\n",
      "            50          -0.37028        0.772\n",
      "            51          -0.36907        0.772\n",
      "            52          -0.36790        0.772\n",
      "            53          -0.36677        0.772\n",
      "            54          -0.36568        0.772\n",
      "            55          -0.36462        0.772\n",
      "            56          -0.36360        0.772\n",
      "            57          -0.36260        0.772\n",
      "            58          -0.36164        0.772\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.36071        0.772\n"
     ]
    }
   ],
   "source": [
    "encoding = nltk.classify.maxent.BinaryMaxentFeatureEncoding.train(train_dict, alwayson_features=True)\n",
    "trained_model = nltk.classify.MaxentClassifier.train(train_dict, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print('Baseline accuracy: ', len([1 for _, y in test_dict if y == 'neg'])/len(test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7063714261474507\n",
      "Precision:  0.9710753440634476\n",
      "Recall:  0.5550666666666667\n",
      "Accuracy:  0.7692666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = trained_model.classify_many([x for x, _ in test_dict])\n",
    "y_true = [y for _, y in test_dict]\n",
    "\n",
    "print('F1: ', f1_score(y_true, y_pred, pos_label='pos'))\n",
    "print('Precision: ', precision_score(y_true, y_pred, pos_label='pos'))\n",
    "print('Recall: ', recall_score(y_true, y_pred, pos_label='pos'))\n",
    "print('Accuracy: ', nltk.classify.accuracy(trained_model, test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme = 'What the fuck did you just fucking say about me, you little bitch? I will have you know I graduated top of my class in the Navy Seals, and I have been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I am the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You are fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that is just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little \"clever\" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you could not, you did not, and now you are paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You are fucking dead, kiddo.'\n",
    "\n",
    "stemmed_meme = []\n",
    "for word in meme.lower().split():\n",
    "    stemmed_meme.append(ps.stem(word))\n",
    "\n",
    "trained_model.classify(Counter(stemmed_meme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to see if TFIDF gives similar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>87940</th>\n",
       "      <td>{'dear': 1, 'mr': 1, 'moodi': 1, 'websit': 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>{'winx': 2, 'club': 2, 'recent': 1, 'edit': 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135136</th>\n",
       "      <td>{'without': 1, 'concern': 1, 'wikipedia': 1, '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11405</th>\n",
       "      <td>{'documentari': 1, 'shown': 1, 'mediacorp': 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23189</th>\n",
       "      <td>{'see': 1, 'need': 1, 'last': 1, 'impact': 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment_text  toxic\n",
       "toxic                                                                 \n",
       "0     87940   {'dear': 1, 'mr': 1, 'moodi': 1, 'websit': 2, ...      0\n",
       "      19563   {'winx': 2, 'club': 2, 'recent': 1, 'edit': 1,...      0\n",
       "      135136  {'without': 1, 'concern': 1, 'wikipedia': 1, '...      0\n",
       "      11405   {'documentari': 1, 'shown': 1, 'mediacorp': 1,...      0\n",
       "      23189   {'see': 1, 'need': 1, 'last': 1, 'impact': 1, ...      0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = comments_subset.copy()\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    {'dear': 104, 'mr': 177, 'moodi': 2, 'websit':...\n",
       "1    {'niggah': 3, 'hey': 793, 'fool': 372, 'whyd':...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf.reset_index(drop=True).groupby('toxic').comment_text.apply(list).apply(lambda counter_list: sum(counter_list, Counter()))\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numDocsWithWord(word, trainList):\n",
    "    count = 0\n",
    "    for doc in trainList:\n",
    "        if word in doc:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(tf, all_words):\n",
    "    idf = dict()\n",
    "    for word in all_words:\n",
    "        idf[word] = log((1+len(tf))/(1+numDocsWithWord(word, tf)))+1\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf, idf):\n",
    "    tfidf = tf.apply(lambda word_dict: {word: tf_val * idf[word] for word, tf_val in word_dict.items()})\n",
    "    tfidf = pd.DataFrame(tfidf)\n",
    "    tfidf['mag'] = tfidf.comment_text.apply(lambda word_dict: sqrt(sum([x**2 for x in word_dict.values()])))\n",
    "    tfidf['wordsNorm'] = tfidf.apply(lambda row: {word:tfidf_score/row.mag for word, tfidf_score in row.comment_text.items()}, axis=1)\n",
    "    normedTfidf = pd.DataFrame(list(tfidf.wordsNorm))\n",
    "    return normedTfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(tf, set(tf[0]) | set(tf[1]))\n",
    "tfidf = compute_tfidf(tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'articl',\n",
       " 'page',\n",
       " 'wikipedia',\n",
       " 'suck',\n",
       " 'edit',\n",
       " 'talk',\n",
       " 'use',\n",
       " 'go',\n",
       " 'shit']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf.max(axis=0).sort_values(ascending=False)[:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'shit',\n",
       " 'articl',\n",
       " 'thank',\n",
       " 'ass',\n",
       " 'suck',\n",
       " 'bitch',\n",
       " 'stupid',\n",
       " 'pleas',\n",
       " 'talk']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF and MaxEnt gave pretty similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity = {'4r5e','5h1t','5hit','a55','anal','anus','ar5e','arrse','arse','ass','ass-fucker','asses','assfucker','assfukka','asshole','assholes','asswhole','a_s_s','b!tch','b00bs','b17ch','b1tch','ballbag','balls','ballsack','bastard','beastial','beastiality','bellend','bestial','bestiality','bi+ch','biatch','bitch','bitcher','bitchers','bitches','bitchin','bitching','bloody','blow job','blowjob','blowjobs','boiolas','bollock','bollok','boner','boob','boobs','booobs','boooobs','booooobs','booooooobs','breasts','buceta','bugger','bum','bunny fucker','butt','butthole','buttmuch','buttplug','c0ck','c0cksucker','carpet muncher','cawk','chink','cipa','cl1t','clit','clitoris','clits','cnut','cock','cock-sucker','cockface','cockhead','cockmunch','cockmuncher','cocks','cocksuck ','cocksucked ','cocksucker','cocksucking','cocksucks ','cocksuka','cocksukka','cok','cokmuncher','coksucka','coon','cox','crap','cum','cummer','cumming','cums','cumshot','cunilingus','cunillingus','cunnilingus','cunt','cuntlick ','cuntlicker ','cuntlicking ','cunts','cyalis','cyberfuc','cyberfuck ','cyberfucked ','cyberfucker','cyberfuckers','cyberfucking ','d1ck','damn','dick','dickhead','dildo','dildos','dink','dinks','dirsa','dlck','dog-fucker','doggin','dogging','donkeyribber','doosh','duche','dyke','ejaculate','ejaculated','ejaculates ','ejaculating ','ejaculatings','ejaculation','ejakulate','f u c k','f u c k e r','f4nny','fag','fagging','faggitt','faggot','faggs','fagot','fagots','fags','fanny','fannyflaps','fannyfucker','fanyy','fatass','fcuk','fcuker','fcuking','feck','fecker','felching','fellate','fellatio','fingerfuck ','fingerfucked ','fingerfucker ','fingerfuckers','fingerfucking ','fingerfucks ','fistfuck','fistfucked ','fistfucker ','fistfuckers ','fistfucking ','fistfuckings ','fistfucks ','flange','fook','fooker','fuck','fucka','fucked','fucker','fuckers','fuckhead','fuckheads','fuckin','fucking','fuckings','fuckingshitmotherfucker','fuckme ','fucks','fuckwhit','fuckwit','fudge packer','fudgepacker','fuk','fuker','fukker','fukkin','fuks','fukwhit','fukwit','fux','fux0r','f_u_c_k','gangbang','gangbanged ','gangbangs ','gaylord','gaysex','goatse','God','god-dam','god-damned','goddamn','goddamned','hardcoresex ','hell','heshe','hoar','hoare','hoer','homo','hore','horniest','horny','hotsex','jack-off ','jackoff','jap','jerk-off ','jism','jiz ','jizm ','jizz','kawk','knob','knobead','knobed','knobend','knobhead','knobjocky','knobjokey','kock','kondum','kondums','kum','kummer','kumming','kums','kunilingus','l3i+ch','l3itch','labia','lmfao','lust','lusting','m0f0','m0fo','m45terbate','ma5terb8','ma5terbate','masochist','master-bate','masterb8','masterbat*','masterbat3','masterbate','masterbation','masterbations','masturbate','mo-fo','mof0','mofo','mothafuck','mothafucka','mothafuckas','mothafuckaz','mothafucked ','mothafucker','mothafuckers','mothafuckin','mothafucking ','mothafuckings','mothafucks','mother fucker','motherfuck','motherfucked','motherfucker','motherfuckers','motherfuckin','motherfucking','motherfuckings','motherfuckka','motherfucks','muff','mutha','muthafecker','muthafuckker','muther','mutherfucker','n1gga','n1gger','nazi','nigg3r','nigg4h','nigga','niggah','niggas','niggaz','nigger','niggers ','nob','nob jokey','nobhead','nobjocky','nobjokey','numbnuts','nutsack','orgasim ','orgasims ','orgasm','orgasms ','p0rn','pawn','pecker','penis','penisfucker','phonesex','phuck','phuk','phuked','phuking','phukked','phukking','phuks','phuq','pigfucker','pimpis','piss','pissed','pisser','pissers','pisses ','pissflaps','pissin ','pissing','pissoff ','poop','porn','porno','pornography','pornos','prick','pricks ','pron','pube','pusse','pussi','pussies','pussy','pussys ','rectum','retard','rimjaw','rimming','s hit','s.o.b.','sadist','schlong','screwing','scroat','scrote','scrotum','semen','sex','sh!+','sh!t','sh1t','shag','shagger','shaggin','shagging','shemale','shi+','shit','shitdick','shite','shited','shitey','shitfuck','shitfull','shithead','shiting','shitings','shits','shitted','shitter','shitters ','shitting','shittings','shitty ','skank','slut','sluts','smegma','smut','snatch','son-of-a-bitch','spac','spunk','s_h_i_t','t1tt1e5','t1tties','teets','teez','testical','testicle','tit','titfuck','tits','titt','tittie5','tittiefucker','titties','tittyfuck','tittywank','titwank','tosser','turd','tw4t','twat','twathead','twatty','twunt','twunter','v14gra','v1gra','vagina','viagra','vulva','w00se','wang','wank','wanker','wanky','whoar','whore','willies','willy','xrated','xxx'}\n",
    "stemmed_prof = set()\n",
    "for word in profanity:\n",
    "    stemmed_word = ps.stem(word)\n",
    "    stemmed_prof.add(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x, y in comments_subset.reset_index(drop=True).iterrows():\n",
    "    y_true.append(y[1])\n",
    "    profane = False\n",
    "    for word in y[0]:\n",
    "        if word in stemmed_prof:\n",
    "            profane = True\n",
    "    if profane:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7837\n",
      "Precision:  0.7837\n",
      "Recall:  0.7837\n",
      "Accuracy:  0.7837\n"
     ]
    }
   ],
   "source": [
    "print('F1: ', f1_score(y_true, y_pred, average='micro'))\n",
    "print('Precision: ', precision_score(y_true, y_pred, average='micro'))\n",
    "print('Recall: ', recall_score(y_true, y_pred, average='micro'))\n",
    "print('Accuracy: ', sum(np.asarray(y_true) == np.asarray(y_pred))/len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple profanity filter was better at finding `toxic` comments than a MaxEnt classifier when looking at accuracy. The recall is higher but the precision is lower for this simple classifier.\n",
    "\n",
    "That means that we are finding more of the toxic comments, however we are classifying more non-toxic comments as toxic. This is weird that non-toxic words contain profanity, however maybe they are quoting something or writing about a controversial topic.\n",
    "\n",
    "Precision = $\\frac{tp}{tp+fp}$\n",
    "\n",
    "Recall = $\\frac{tp}{tp+fn}$\n",
    "\n",
    "This means that $fp = fn$\n",
    "\n",
    "What else is weird is that all the numbers are exactly the same. This must mean that the distribution of profanity in toxic comments is the inverse probability of finding profanity in nontoxic comments. Or that I am doing something completely wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity and then word embeddings\n",
    "\n",
    "1. Some Gensim stuff\n",
    "\n",
    "- word embeddings -> PCA/t-SNE for docs\n",
    "\n",
    "- 2 group Latent Direchlet Allocation (unsupervised)\n",
    "\n",
    "- word cooccurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of a bag-of-words model, we could look at:\n",
    "\n",
    "- Length of comment\n",
    "- Capital letters (number/proportion)\n",
    "- Punctuation marks (question/exclamation/etc)\n",
    "- Unique words\n",
    "- Words not in dictionary (trying to get around censoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
